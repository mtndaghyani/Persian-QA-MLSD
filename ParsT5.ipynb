{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "kuTMv_0elgJx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: pytorch_lightning in /home/user01/miniconda3/lib/python3.7/site-packages (1.9.4)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from pytorch_lightning) (4.64.1)\n",
      "Requirement already satisfied: packaging>=17.1 in /home/user01/miniconda3/lib/python3.7/site-packages (from pytorch_lightning) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from pytorch_lightning) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/user01/miniconda3/lib/python3.7/site-packages (from pytorch_lightning) (1.21.6)\n",
      "Requirement already satisfied: lightning-utilities>=0.6.0.post0 in /home/user01/miniconda3/lib/python3.7/site-packages (from pytorch_lightning) (0.8.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /home/user01/miniconda3/lib/python3.7/site-packages (from pytorch_lightning) (6.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from pytorch_lightning) (0.11.4)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from pytorch_lightning) (2023.1.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from pytorch_lightning) (1.13.1+cu116)\n",
      "Requirement already satisfied: requests in /home/user01/miniconda3/lib/python3.7/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.28.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/user01/miniconda3/lib/python3.7/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.0.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from lightning-utilities>=0.6.0.post0->pytorch_lightning) (4.11.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/user01/miniconda3/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/user01/miniconda3/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/user01/miniconda3/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/user01/miniconda3/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.8.2)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (0.13.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/user01/miniconda3/lib/python3.7/site-packages (from importlib-metadata>=4.0.0->lightning-utilities>=0.6.0.post0->pytorch_lightning) (3.11.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/user01/miniconda3/lib/python3.7/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user01/miniconda3/lib/python3.7/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user01/miniconda3/lib/python3.7/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: nlp in /home/user01/miniconda3/lib/python3.7/site-packages (0.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from nlp) (2.28.1)\n",
      "Requirement already satisfied: pandas in /home/user01/miniconda3/lib/python3.7/site-packages (from nlp) (1.3.5)\n",
      "Requirement already satisfied: filelock in /home/user01/miniconda3/lib/python3.7/site-packages (from nlp) (3.9.0)\n",
      "Requirement already satisfied: xxhash in /home/user01/miniconda3/lib/python3.7/site-packages (from nlp) (3.2.0)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from nlp) (12.0.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/user01/miniconda3/lib/python3.7/site-packages (from nlp) (4.64.1)\n",
      "Requirement already satisfied: numpy in /home/user01/miniconda3/lib/python3.7/site-packages (from nlp) (1.21.6)\n",
      "Requirement already satisfied: dill in /home/user01/miniconda3/lib/python3.7/site-packages (from nlp) (0.3.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/user01/miniconda3/lib/python3.7/site-packages (from requests>=2.19.0->nlp) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user01/miniconda3/lib/python3.7/site-packages (from requests>=2.19.0->nlp) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/user01/miniconda3/lib/python3.7/site-packages (from requests>=2.19.0->nlp) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user01/miniconda3/lib/python3.7/site-packages (from requests>=2.19.0->nlp) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/user01/miniconda3/lib/python3.7/site-packages (from pandas->nlp) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/user01/miniconda3/lib/python3.7/site-packages (from pandas->nlp) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/user01/miniconda3/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->nlp) (1.16.0)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /home/user01/miniconda3/lib/python3.7/site-packages (4.28.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: filelock in /home/user01/miniconda3/lib/python3.7/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user01/miniconda3/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/user01/miniconda3/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: requests in /home/user01/miniconda3/lib/python3.7/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/user01/miniconda3/lib/python3.7/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/user01/miniconda3/lib/python3.7/site-packages (from transformers) (4.11.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/user01/miniconda3/lib/python3.7/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/user01/miniconda3/lib/python3.7/site-packages (from transformers) (2023.5.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/user01/miniconda3/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: fsspec in /home/user01/miniconda3/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/user01/miniconda3/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user01/miniconda3/lib/python3.7/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user01/miniconda3/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/user01/miniconda3/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/user01/miniconda3/lib/python3.7/site-packages (from requests->transformers) (2.0.4)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: flax in /home/user01/miniconda3/lib/python3.7/site-packages (0.6.4)\n",
      "Requirement already satisfied: msgpack in /home/user01/miniconda3/lib/python3.7/site-packages (from flax) (1.0.5)\n",
      "Requirement already satisfied: optax in /home/user01/miniconda3/lib/python3.7/site-packages (from flax) (0.1.4)\n",
      "Requirement already satisfied: jax>=0.3.16 in /home/user01/miniconda3/lib/python3.7/site-packages (from flax) (0.3.25)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /home/user01/miniconda3/lib/python3.7/site-packages (from flax) (6.0)\n",
      "Requirement already satisfied: tensorstore in /home/user01/miniconda3/lib/python3.7/site-packages (from flax) (0.1.28)\n",
      "Requirement already satisfied: rich>=11.1 in /home/user01/miniconda3/lib/python3.7/site-packages (from flax) (13.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /home/user01/miniconda3/lib/python3.7/site-packages (from flax) (4.4.0)\n",
      "Requirement already satisfied: matplotlib in /home/user01/miniconda3/lib/python3.7/site-packages (from flax) (3.5.3)\n",
      "Requirement already satisfied: orbax in /home/user01/miniconda3/lib/python3.7/site-packages (from flax) (0.1.0)\n",
      "Requirement already satisfied: numpy>=1.12 in /home/user01/miniconda3/lib/python3.7/site-packages (from flax) (1.21.6)\n",
      "Requirement already satisfied: scipy>=1.5 in /home/user01/miniconda3/lib/python3.7/site-packages (from jax>=0.3.16->flax) (1.7.3)\n",
      "Requirement already satisfied: opt-einsum in /home/user01/miniconda3/lib/python3.7/site-packages (from jax>=0.3.16->flax) (3.3.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from rich>=11.1->flax) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from rich>=11.1->flax) (2.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/user01/miniconda3/lib/python3.7/site-packages (from matplotlib->flax) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/user01/miniconda3/lib/python3.7/site-packages (from matplotlib->flax) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/user01/miniconda3/lib/python3.7/site-packages (from matplotlib->flax) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from matplotlib->flax) (9.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from matplotlib->flax) (23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/user01/miniconda3/lib/python3.7/site-packages (from matplotlib->flax) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from matplotlib->flax) (4.38.0)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /home/user01/miniconda3/lib/python3.7/site-packages (from optax->flax) (1.4.0)\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in /home/user01/miniconda3/lib/python3.7/site-packages (from optax->flax) (0.3.25)\n",
      "Requirement already satisfied: chex>=0.1.5 in /home/user01/miniconda3/lib/python3.7/site-packages (from optax->flax) (0.1.5)\n",
      "Requirement already satisfied: importlib_resources in /home/user01/miniconda3/lib/python3.7/site-packages (from orbax->flax) (5.12.0)\n",
      "Requirement already satisfied: pytest in /home/user01/miniconda3/lib/python3.7/site-packages (from orbax->flax) (7.3.1)\n",
      "Requirement already satisfied: etils in /home/user01/miniconda3/lib/python3.7/site-packages (from orbax->flax) (0.9.0)\n",
      "Requirement already satisfied: cached_property in /home/user01/miniconda3/lib/python3.7/site-packages (from orbax->flax) (1.5.2)\n",
      "Requirement already satisfied: dm-tree>=0.1.5 in /home/user01/miniconda3/lib/python3.7/site-packages (from chex>=0.1.5->optax->flax) (0.1.8)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from chex>=0.1.5->optax->flax) (0.12.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/user01/miniconda3/lib/python3.7/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/user01/miniconda3/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->flax) (1.16.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from importlib_resources->orbax->flax) (3.11.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /home/user01/miniconda3/lib/python3.7/site-packages (from pytest->orbax->flax) (1.0.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/user01/miniconda3/lib/python3.7/site-packages (from pytest->orbax->flax) (2.0.1)\n",
      "Requirement already satisfied: iniconfig in /home/user01/miniconda3/lib/python3.7/site-packages (from pytest->orbax->flax) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /home/user01/miniconda3/lib/python3.7/site-packages (from pytest->orbax->flax) (4.11.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /home/user01/miniconda3/lib/python3.7/site-packages (from pytest->orbax->flax) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_lightning\n",
    "!pip install nlp\n",
    "!pip install transformers\n",
    "!pip install flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hGhn0Zolk65R",
    "outputId": "eb723628-ca56-4766-a351-dcbc4328b762"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/miniconda3/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/user01/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "import textwrap\n",
    "from tqdm.auto import tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from nlp import load_metric\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uBXQjOU_rfI2"
   },
   "outputs": [],
   "source": [
    "train_path = 'train.csv'\n",
    "test_path  = 'validation.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JUUylwwNAOVo",
    "outputId": "726a0eb5-b26a-4d2f-cd70-3d9817dec523"
   },
   "outputs": [],
   "source": [
    "\n",
    "def concatenate(row):\n",
    "    return 'متن: ' + row['context'] + '، پرسش: ' + row['question']\n",
    "\n",
    "def format_df(k):\n",
    "    # Apply the function to create a new 'input' column\n",
    "    k['input'] = k.apply(concatenate, axis=1)\n",
    "\n",
    "    # Extract the first answer from the 'answers' column\n",
    "    k['output'] = k['answers']\n",
    "\n",
    "    # Create a new DataFrame with 'input' and 'output' columns\n",
    "    df = pd.DataFrame({'input': k['input'], 'output': k['output']})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = format_df(train_df)\n",
    "test_data = format_df(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  input  \\\n",
      "0     متن: نورمن ها (نورمن: Nourmands ؛ فرانسوی: Nor...   \n",
      "1     متن: نورمن ها (نورمن: Nourmands ؛ فرانسوی: Nor...   \n",
      "2     متن: نورمن ها (نورمن: Nourmands ؛ فرانسوی: Nor...   \n",
      "3     متن: سلسله نورمن تأثیر عمده سیاسی ، فرهنگی و ن...   \n",
      "4     متن: سلسله نورمن تأثیر عمده سیاسی ، فرهنگی و ن...   \n",
      "...                                                 ...   \n",
      "3230  متن: هندوانه یا تربوز (نام علمی: Citrullus lan...   \n",
      "3231  متن: هندوانه یا تربوز (نام علمی: Citrullus lan...   \n",
      "3232  متن: هندوانه یا تربوز (نام علمی: Citrullus lan...   \n",
      "3233  متن: هندوانه یا تربوز (نام علمی: Citrullus lan...   \n",
      "3234  متن: هندوانه یا تربوز (نام علمی: Citrullus lan...   \n",
      "\n",
      "                                                 output  \n",
      "0                                                فرانسه  \n",
      "1                                          قرون 10 و 11  \n",
      "2                               دانمارک ، ایسلند و نروژ  \n",
      "3                                           ویلیام فاتح  \n",
      "4                                            ریچارد اول  \n",
      "...                                                 ...  \n",
      "3230            شته، مگس خربزه و کرم لوله‌ای ریشه‌گرهی.  \n",
      "3231  توسط دانشمندان ژاپنی که قادر به تولید دورگه تر...  \n",
      "3232  با قرار دادن میوه‌ها در جعبه‌هایی فلزی و شیشه‌...  \n",
      "3233  هندوانه گیاهی گرمسیری یا نیمه گرمسیری است و بر...  \n",
      "3234  دانه‌ها معمولاً در گلدان‌هایی دارای پوشش کاشته...  \n",
      "\n",
      "[3235 rows x 2 columns]0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user01/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3361</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_loc</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3358 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3359 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>casted_key = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._maybe_cast_indexer(key)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3360 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3361 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._engine.get_loc(casted_key)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3362 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyError</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> err:                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3363 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyError</span>(key) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">err</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3364 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pandas._libs.index.IndexEngine.get_loc</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">76</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pandas._libs.index.IndexEngine.get_loc</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">108</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pandas._libs.hashtable.PyObjectHashTable.get_item</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5198</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pandas._libs.hashtable.PyObjectHashTable.get_item</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5206</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>\n",
       "\n",
       "<span style=\"font-style: italic\">The above exception was the direct cause of the following exception:</span>\n",
       "\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 test_data[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>]                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user01/miniconda3/lib/python3.7/site-packages/pandas/core/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">frame.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3458</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__getitem__</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3455 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> is_single_key:                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3456 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.columns.nlevels &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>:                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3457 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._getitem_multilevel(key)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 3458 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>indexer = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.columns.get_loc(key)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3459 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> is_integer(indexer):                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3460 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>indexer = [indexer]                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3461 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/user01/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3363</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_loc</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3360 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3361 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._engine.get_loc(casted_key)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3362 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyError</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> err:                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3363 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyError</span>(key) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">err</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3364 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3365 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> is_scalar(key) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> isna(key) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.hasnans:                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3366 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyError</span>(key)                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user01/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m3361\u001b[0m in \u001b[92mget_loc\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3358 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3359 \u001b[0m\u001b[2m│   │   │   \u001b[0mcasted_key = \u001b[96mself\u001b[0m._maybe_cast_indexer(key)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3360 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3361 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._engine.get_loc(casted_key)                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3362 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mKeyError\u001b[0m \u001b[94mas\u001b[0m err:                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3363 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mKeyError\u001b[0m(key) \u001b[94mfrom\u001b[0m \u001b[4;96merr\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3364 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mpandas._libs.index.IndexEngine.get_loc\u001b[0m:\u001b[94m76\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mpandas._libs.index.IndexEngine.get_loc\u001b[0m:\u001b[94m108\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0m:\u001b[94m5198\u001b[0m                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0m:\u001b[94m5206\u001b[0m                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyError: \u001b[0m\u001b[32m'0'\u001b[0m\n",
       "\n",
       "\u001b[3mThe above exception was the direct cause of the following exception:\u001b[0m\n",
       "\n",
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 test_data[\u001b[96minput\u001b[0m]                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user01/miniconda3/lib/python3.7/site-packages/pandas/core/\u001b[0m\u001b[1;33mframe.py\u001b[0m:\u001b[94m3458\u001b[0m in \u001b[92m__getitem__\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3455 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m is_single_key:                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3456 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.columns.nlevels > \u001b[94m1\u001b[0m:                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3457 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._getitem_multilevel(key)                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 3458 \u001b[2m│   │   │   \u001b[0mindexer = \u001b[96mself\u001b[0m.columns.get_loc(key)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3459 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m is_integer(indexer):                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3460 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mindexer = [indexer]                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3461 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/user01/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m3363\u001b[0m in \u001b[92mget_loc\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3360 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3361 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._engine.get_loc(casted_key)                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3362 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mKeyError\u001b[0m \u001b[94mas\u001b[0m err:                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3363 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mKeyError\u001b[0m(key) \u001b[94mfrom\u001b[0m \u001b[4;96merr\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3364 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3365 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m is_scalar(key) \u001b[95mand\u001b[0m isna(key) \u001b[95mand\u001b[0m \u001b[95mnot\u001b[0m \u001b[96mself\u001b[0m.hasnans:                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3366 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mKeyError\u001b[0m(key)                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyError: \u001b[0m\u001b[32m'0'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data[input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DUFeP91IfwK"
   },
   "outputs": [],
   "source": [
    "df = df.iloc[50000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "G67Cqwi_sV19"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rIfgljPgQjAA"
   },
   "outputs": [],
   "source": [
    "class bigbang(Dataset):\n",
    "    def __init__(self, df, tokenizer, input_length, output_length):         \n",
    "        self.dataset =  df\n",
    "        self.input_length = input_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.output_length = output_length\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "    def clean_text(self, text):\n",
    "        text = text.replace('\\n','')\n",
    "        return text\n",
    "    def convert_to_features(self, example_batch):\n",
    "        input_ = self.clean_text(example_batch['input'])\n",
    "        target_ = self.clean_text(example_batch['output'])\n",
    "        source = self.tokenizer.batch_encode_plus([input_], max_length=self.input_length, \n",
    "                                                     padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "        targets = self.tokenizer.batch_encode_plus([target_], max_length=self.output_length, \n",
    "                                                     padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "        return source, targets\n",
    "    def __getitem__(self, index):\n",
    "        source, targets = self.convert_to_features(self.dataset.iloc[index])\n",
    "        source_ids = source[\"input_ids\"].squeeze()\n",
    "        target_ids = targets[\"input_ids\"].squeeze()\n",
    "        src_mask    = source[\"attention_mask\"].squeeze()\n",
    "        target_mask = targets[\"attention_mask\"].squeeze()\n",
    "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mx0h_SPiKEY8"
   },
   "outputs": [],
   "source": [
    "model_path = 'parsT5'\n",
    "configs = {\n",
    "  \"local_files_only\": True,\n",
    "  \"from_flax\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lfs\n",
      "  Downloading lfs-0.2-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: lfs\n",
      "Successfully installed lfs-0.2\n",
      "fatal: destination path 'parsT5' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!pip  install lfs\n",
    "!git clone https://huggingface.co/Ahmad/parsT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0rvcHiU4eFHA",
    "outputId": "6b7feec4-b367-4698-9c05-3d9f6eb06133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at ./drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('./drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nS3dfqbgRGn-",
    "outputId": "c3e8c79e-d725-412e-a381-246042c09850"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 tokenizer = AutoTokenizer.from_pretrained(model_path, **configs)<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#('Ahmad/parsT5-base')#(</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>dataset = bigbang(train_data, tokenizer, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">512</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">150</span>)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(dataset)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'model_path'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 tokenizer = AutoTokenizer.from_pretrained(model_path, **configs)\u001b[2m#('Ahmad/parsT5-base')#(\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0mdataset = bigbang(train_data, tokenizer, \u001b[94m512\u001b[0m, \u001b[94m150\u001b[0m)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m\u001b[96mlen\u001b[0m(dataset)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'model_path'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, **configs)#('Ahmad/parsT5-base')#('google/mt5-base')\n",
    "dataset = bigbang(train_data, tokenizer, 512, 150)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LmaEBH0QRKv4",
    "outputId": "d807ea55-2733-409d-af14-4e87370fe230"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of Tokenized Text:  torch.Size([512])\n",
      "\n",
      "Sanity check - Decode Text:  متن: در 4 آوریل 2008 ، بیانسه با جی ز ازدواج کرد. او ازدواج خود را در مونتاژ ویدیویی در مهمانی گوش دادن به سومین آلبوم استودیویی خود ، من هستم... ساشا فیرس ، در 22 دسامبر 2008 در باشگاه سونی منهتن آشکار کرد. من هستم... ساشا فیرس در 18 نوامبر 2008 در ایالات متحده آزاد شد. این آلبوم رسماً آلترایگو بیانسه را معرفی می کند ، ساشا فییرس ، که هنگام ساخت تک آهنگ \"دیوانه عاشق\" در سال 2003 تصور شد ، در هفته اول 482،000 نسخه فروش ، اولین نمایش در بالای بیلبورد 200 ، و سومین آلبوم شماره یک پی در پی به بیانسه در آمریکا این آلبوم شامل آهنگ شماره یک \"single ladies (put a ring on it)\" و پنج آهنگ برتر \"if i are a boy\" و \"halo\" بود. موفقیت \"halo\" در ایالات متحده با دستیابی به موفقیت در تبدیل شدن به طولانی ترین تک آهنگ داغ 100 خود در کارنامه خود ، به بیانسه کمک کرد تا در طول دهه 2000 بیش از هر زن دیگری در لیست 10 تک آهنگ برتر این لیست قرار گیرد. این فیلم همچنین شامل \"رویاهای شیرین\" و تک آهنگهای \"diva\" ، \"ego\" ، \"girl broken-hearted\" و \"video video\" بود. نماهنگ \"single ladies\" در سراسر جهان تقلید و تقلید شده است و بر اساس گزارش تورنتو استار \"اولین دیوانه رقص بزرگ\" عصر اینترنت است. این ویدئو چندین جایزه از جمله بهترین ویدئو در جوایز موسیقی mtv europe 2009 ، جوایز 2009 اسکاتلندی mobo و جوایز bet 2009 را از آن خود کرده است. در جوایز موسیقی mtv video music 2009 ، این ویدئو برای 9 جایزه نامزد شد که در نهایت سه جایزه از جمله فیلم سال را از آن خود کرد. عدم موفقیت در کسب بهترین دسته ویدیوی زن ، که به \"you belong with me\" خواننده پاپ کشور آمریکایی تیلور سوئیفت تعلق گرفت ، باعث شد که</s>\n",
      "====================================\n",
      "Sanity check - Decode Summary:  119.5 میلیون</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "data = dataset[50]\n",
    "print()\n",
    "print(\"Shape of Tokenized Text: \", data['source_ids'].shape)\n",
    "print()\n",
    "print(\"Sanity check - Decode Text: \", tokenizer.decode(data['source_ids']))\n",
    "print(\"====================================\")\n",
    "print(\"Sanity check - Decode Summary: \", tokenizer.decode(data['target_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "vutwd9MyYupu"
   },
   "outputs": [],
   "source": [
    "def get_dataset(df, tokenizer, args):\n",
    "      return bigbang(df, tokenizer=tokenizer, input_length=args.max_input_length, \n",
    "                        output_length=args.max_output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LN3h98Tf6tRl"
   },
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(T5FineTuner, self).__init__()\n",
    "        self.hparamss = hparams        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"model2\")\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\"model2\")\n",
    "        # self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
    "#         self.rouge_metric = load_metric('rouge')  \n",
    "        n_observations_per_split = {\n",
    "            \"train\": self.hparamss.n_train,\n",
    "            \"validation\": self.hparamss.n_val,\n",
    "            \"test\": self.hparamss.n_test,\n",
    "        }\n",
    "        self.n_obs = {k: v if v >= 0 else None for k, v in n_observations_per_split.items()}\n",
    "    def freeze_params(self, model):\n",
    "        for par in model.parameters():\n",
    "            par.requires_grad = False\n",
    "    def lmap(self, f, x):\n",
    "        \"\"\"list(map(f, x))\"\"\"\n",
    "        return list(map(f, x))\n",
    "    def is_logger(self):\n",
    "        return self.trainer.global_rank <= 0\n",
    "    def parse_score(self, result):\n",
    "        return {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()} \n",
    "    def forward(\n",
    "      self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None\n",
    "  ):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels,\n",
    "    )\n",
    "    def _step(self, batch):\n",
    "        labels = batch[\"target_ids\"]\n",
    "        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "        outputs = self(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            labels=labels,\n",
    "            decoder_attention_mask=batch['target_mask'])\n",
    "        loss = outputs[0]\n",
    "        return loss\n",
    "    def ids_to_clean_text(self, generated_ids):\n",
    "        gen_text = self.tokenizer.batch_decode(\n",
    "            generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        return self.lmap(str.strip, gen_text)\n",
    "    def _generative_step(self, batch) :\n",
    "        t0 = time.time()\n",
    "        generated_ids = self.model.generate(\n",
    "            batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            use_cache=True,\n",
    "            decoder_attention_mask=batch['target_mask'],\n",
    "            max_length=150, \n",
    "            num_beams=2,\n",
    "            repetition_penalty=2.5, \n",
    "            length_penalty=1.0, \n",
    "            early_stopping=True)\n",
    "        preds = self.ids_to_clean_text(generated_ids)\n",
    "        target = self.ids_to_clean_text(batch[\"target_ids\"]) \n",
    "        gen_time = (time.time() - t0) / batch[\"source_ids\"].shape[0]  \n",
    "        loss = self._step(batch)\n",
    "        base_metrics = {'val_loss': loss}\n",
    "        summ_len = np.mean(self.lmap(len, generated_ids))\n",
    "        base_metrics.update(gen_time=gen_time, gen_len=summ_len, preds=preds, target=target)\n",
    "#         self.rouge_metric.add_batch(preds, target)\n",
    "        return base_metrics\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        tensorboard_logs = {\"train_loss\": loss}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._generative_step(batch)\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "#         rouge_results = self.rouge_metric.compute() \n",
    "#         rouge_dict = self.parse_score(rouge_results)\n",
    "#         tensorboard_logs.update(rouge1=rouge_dict['rouge1'], rougeL=rouge_dict['rougeL'])\n",
    "        self.target_gen= []\n",
    "        self.prediction_gen=[]\n",
    "        return {\"avg_val_loss\": avg_loss, \n",
    "#                 \"rouge1\" : rouge_results['rouge1'],\n",
    "#                 \"rougeL\" : rouge_results['rougeL'],\n",
    "                \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
    "    def configure_optimizers(self):\n",
    "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparamss.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparamss.learning_rate, eps=self.hparamss.adam_epsilon)\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "    def optimizer_step(self,\n",
    "                     epoch=None,\n",
    "                     batch_idx=None,\n",
    "                     optimizer=None,\n",
    "                     optimizer_idx=None,\n",
    "                     optimizer_closure=None,\n",
    "                     on_tpu=None,\n",
    "                     using_native_amp=None,\n",
    "                     using_lbfgs=None):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_closure()\n",
    "        self.lr_scheduler.step()\n",
    "    def get_tqdm_dict(self):\n",
    "        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "        return tqdm_dict\n",
    "    def train_dataloader(self):   \n",
    "        n_samples = self.n_obs['train']\n",
    "        train_dataset = get_dataset(train_data, tokenizer=self.tokenizer, args=self.hparamss)\n",
    "        dataloader = DataLoader(train_dataset, batch_size=self.hparamss.train_batch_size, drop_last=True, shuffle=True, num_workers=2)\n",
    "        t_total = (\n",
    "            (len(dataloader.dataset) // (self.hparamss.train_batch_size * max(1, self.hparamss.n_gpu)))\n",
    "            // self.hparamss.gradient_accumulation_steps\n",
    "            * float(self.hparamss.num_train_epochs)\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.opt, num_warmup_steps=self.hparamss.warmup_steps, num_training_steps=t_total\n",
    "        )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return dataloader\n",
    "    def val_dataloader(self):\n",
    "        n_samples = self.n_obs['validation']\n",
    "        validation_dataset = get_dataset(test_data,tokenizer=self.tokenizer, args=self.hparamss)\n",
    "        return DataLoader(validation_dataset, batch_size=self.hparamss.eval_batch_size, num_workers=2)\n",
    "    def test_dataloader(self):\n",
    "        n_samples = self.n_obs['test']\n",
    "        test_dataset = get_dataset(test_data, tokenizer=self.tokenizer, args=self.hparamss)\n",
    "        return DataLoader(test_dataset, batch_size=self.hparamss.eval_batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5reD83P4vqXF"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "class LoggingCallback(pl.Callback):\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        logger.info(\"***** Validation results *****\")\n",
    "        if pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "            for key in sorted(metrics):\n",
    "                if key not in [\"log\", \"progress_bar\"]:\n",
    "                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "\n",
    "    def on_test_end(self, trainer, pl_module):\n",
    "        logger.info(\"***** Test results *****\")\n",
    "        if pl_module.is_logger():\n",
    "            metrics = trainer.callback_metrics\n",
    "            output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
    "            with open(output_test_results_file, \"w\") as writer:\n",
    "                for key in sorted(metrics):\n",
    "                    if key not in [\"log\", \"progress_bar\"]:\n",
    "                        logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "                        writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "# logger = LoggingCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"logs/\", name=\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "WRq4BwiDvfIy"
   },
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    output_dir=\"model\", \n",
    "    model_name_or_path='Ahmad/parsT5',\n",
    "    tokenizer_name_or_path='Ahmad/parsT5',\n",
    "    max_input_length=512,\n",
    "    max_output_length=150,\n",
    "    freeze_encoder=False,\n",
    "    freeze_embeds=False,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.0,\n",
    "    adam_epsilon=1e-8,\n",
    "    warmup_steps=0,\n",
    "    train_batch_size=8,\n",
    "    eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    n_gpu=1,\n",
    "    resume_from_checkpoint=None, \n",
    "    val_check_interval = 10, \n",
    "    limit_val_batches = 0,\n",
    "    n_val=5,\n",
    "    n_train=-1,\n",
    "    n_test=-1,\n",
    "    early_stop_callback=False,\n",
    "    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
    "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "QpwvZUNvwiwh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p t5_bigbang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hPT6ceyQvbUR",
    "outputId": "fe936187-4d5c-4bda-893c-9d3d0e6f859e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_dir': 'model', 'model_name_or_path': 'Ahmad/parsT5', 'tokenizer_name_or_path': 'Ahmad/parsT5', 'max_input_length': 512, 'max_output_length': 150, 'freeze_encoder': False, 'freeze_embeds': False, 'learning_rate': 0.0003, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'train_batch_size': 8, 'eval_batch_size': 8, 'num_train_epochs': 4, 'gradient_accumulation_steps': 8, 'n_gpu': 1, 'resume_from_checkpoint': None, 'val_check_interval': 10, 'limit_val_batches': 0, 'n_val': 5, 'n_train': -1, 'n_test': -1, 'early_stop_callback': False, 'fp_16': False, 'opt_level': 'O1', 'max_grad_norm': 1.0, 'seed': 42}\n"
     ]
    }
   ],
   "source": [
    "args_dict.update({ 'num_train_epochs':4,\n",
    "                 'train_batch_size': 8, 'eval_batch_size': 8})\n",
    "args = argparse.Namespace(**args_dict)\n",
    "print(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "czgubNOSvV6f"
   },
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=args.output_dir, monitor=\"val_loss\", mode=\"min\", save_top_k=3\n",
    ")\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    gpus=args.n_gpu,\n",
    "    max_epochs=5,\n",
    "    # early_stop_callback=False,\n",
    "    precision= 32,\n",
    "    amp_backend='apex',\n",
    "    amp_level=args.opt_level,\n",
    "    resume_from_checkpoint=args.resume_from_checkpoint,\n",
    "    gradient_clip_val=args.max_grad_norm,\n",
    "    val_check_interval=args.val_check_interval,\n",
    "    num_sanity_val_steps=0 ,\n",
    "    limit_val_batches = 0,\n",
    "    logger=logger,\n",
    "    callbacks=[LoggingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "AhgHhiAlzjmq"
   },
   "outputs": [],
   "source": [
    "model = T5FineTuner(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.save_pretrained('parsT5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GNIigBRw1vq6",
    "outputId": "4e1a1c3c-b50b-4f8d-c0aa-c3b6a05d3045"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:407: LightningDeprecationWarning: The NVIDIA/apex AMP implementation has been deprecated upstream. Consequently, its integration inside PyTorch Lightning has been deprecated in v1.9.0 and will be removed in v2.0.0. The `Trainer(amp_backend='apex')` argument is deprecated. Removing this argument will avoid this message, it will select PyTorch's implementation automatically.\n",
      "  \"The NVIDIA/apex AMP implementation has been deprecated upstream. Consequently, its integration inside\"\n",
      "/home/user01/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:418: LightningDeprecationWarning: The NVIDIA/apex AMP implementation has been deprecated upstream. Consequently, its integration inside PyTorch Lightning has been deprecated in v1.9.0 and will be removed in v2.0.0. The `Trainer(amp_level='O1')` argument is deprecated. Removing this argument will avoid this message.\n",
      "  \"The NVIDIA/apex AMP implementation has been deprecated upstream. Consequently, its integration inside\"\n",
      "/home/user01/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:479: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347,
     "referenced_widgets": [
      "9e62ca451e9146d9a754f43a782fae63",
      "1eb7e082e87a4391af2f1709b922026e",
      "0d325e1310554dcd9ec1621da7aad860",
      "02236697a1c742c39aa921bfde36e8c8",
      "2b8905ab953943ba8bf02099a8ce2382",
      "d835b21a1f8643d4afb32513abe0adb7",
      "ea1d3177d5c6489f974569763842a8d0",
      "86c7ae40504a4ea484d4559c8eabd6a1",
      "daad5d1dbd3a463abc9769a402e78326",
      "b396aabd83374914874d02f0e50acfd9",
      "74fbcec5dec04d84b906aa41f7b209b6"
     ]
    },
    "id": "_deAgQDB2CSq",
    "outputId": "ed98ecdd-1eca-4d22-cc00-e5d5ed82a5fd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:93: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch (rather, they are called on every optimization step).\n",
      "  \"When using `Trainer(accumulate_grad_batches != 1)` and overriding\"\n",
      "You are using a CUDA device ('GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/user01/miniconda3/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 247 M \n",
      "-----------------------------------------------------\n",
      "247 M     Trainable params\n",
      "0         Non-trainable params\n",
      "247 M     Total params\n",
      "990.158   Total estimated model params size (MB)\n",
      "/home/user01/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:229: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                         | 0/6196 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 0:   0%|          | 7/6196 [00:04<1:00:17,  1.71it/s, loss=0.831, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/miniconda3/lib/python3.7/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:363: LightningDeprecationWarning: The NVIDIA/apex AMP implementation has been deprecated upstream. Consequently, its integration inside PyTorch Lightning has been deprecated in v1.9.0 and will be removed in v2.0.0. The `T5FineTuner.optimizer_step()` hook is overridden, including the `using_native_amp` argument. Removing this argument will avoid this message, you can expect it to return True.\n",
      "  \"The NVIDIA/apex AMP implementation has been deprecated upstream. Consequently, its integration inside\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                     | 0/6196 [00:00<?, ?it/s, loss=1.01, v_num=1]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 2:   0%|                     | 0/6196 [00:00<?, ?it/s, loss=1.16, v_num=1]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 3:   0%|                    | 0/6196 [00:00<?, ?it/s, loss=0.765, v_num=1]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 4:   0%|                    | 0/6196 [00:00<?, ?it/s, loss=0.633, v_num=1]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 4: 100%|█████████| 6196/6196 [39:17<00:00,  2.63it/s, loss=0.767, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|█████████| 6196/6196 [39:32<00:00,  2.61it/s, loss=0.767, v_num=1]\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fW-0_ULdSyw4",
    "outputId": "ac5dcfe1-eed9-43e0-f985-4eedba60acb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_final/tokenizer_config.json',\n",
       " './model_final/special_tokens_map.json',\n",
       " './model_final/tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!mkdir model_final\n",
    "model.model.save_pretrained('./model_final')\n",
    "model.tokenizer.save_pretrained('./model_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5TokenizerFast(name_or_path='./model2', vocab_size=32003, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model2.model.from_pretrained('./model2')\n",
    "model2.tokenizer.from_pretrained('./model2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdxlTjdoCwdM"
   },
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPRklSDBSyw5"
   },
   "outputs": [],
   "source": [
    "df = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "aAFJzvioCydb"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./model_final')\n",
    "model = T5ForConditionalGeneration.from_pretrained('./model_final')\n",
    "dataset = bigbang(test_data, tokenizer, 512, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model2/tokenizer_config.json',\n",
       " './model2/special_tokens_map.json',\n",
       " './model2/tokenizer.json')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "UQWK-GjIDSCg"
   },
   "outputs": [],
   "source": [
    "model.to('cuda')\n",
    "dec, targets = [], []\n",
    "for i in range(len(test_data)//32):\n",
    "    loader = DataLoader(dataset, 32, shuffle=True)\n",
    "    it = iter(loader)\n",
    "    batch = next(it)\n",
    "    batch[\"source_ids\"].shape\n",
    "    outs = model.generate(\n",
    "                batch[\"source_ids\"].cuda(),\n",
    "                attention_mask=batch[\"source_mask\"].cuda(),\n",
    "                use_cache=True,\n",
    "                decoder_attention_mask=batch['target_mask'].cuda(),\n",
    "                max_length=150, \n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5, \n",
    "                length_penalty=1.0, \n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "    dec = dec + [tokenizer.decode(ids) for ids in outs]\n",
    "\n",
    "# texts = [tokenizer.decode(ids) for ids in batch['source_ids']]\n",
    "    targets = targets + [tokenizer.decode(ids) for ids in batch['target_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "yYqn4AUuWnSu"
   },
   "outputs": [],
   "source": [
    "d_test = pd.DataFrame()\n",
    "d_test['pred'] = dec\n",
    "d_test['target'] = targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "3OX97xkCWpmT"
   },
   "outputs": [],
   "source": [
    "d_test['pred'] = [i.replace('<pad>', '').replace('</s>', '').strip() for i in d_test['pred']]\n",
    "d_test['target'] = [i.replace('<pad>', '').replace('</s>', '') for i in d_test['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "R5_PcnmBFqPu",
    "outputId": "8348ef13-e92a-4119-a1bc-299ef4df5f5c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>socal</td>\n",
       "      <td>socal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ویرجینیا</td>\n",
       "      <td>شرکت اوهایو ویرجینیا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22 نوامبر</td>\n",
       "      <td>22 نوامبر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>نورمن ها و نورمن</td>\n",
       "      <td>نورمن</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>دلتا</td>\n",
       "      <td>راین قدیمی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3227</th>\n",
       "      <td>منطقه ای در جنوب شرق اروپا</td>\n",
       "      <td>منطقه ای در جنوب شرق اروپا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3228</th>\n",
       "      <td>باکتریهای بی هوازی</td>\n",
       "      <td>باکتریهای بی هوازی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3229</th>\n",
       "      <td>دهه 1960</td>\n",
       "      <td>دهه 1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>ورشو</td>\n",
       "      <td>ورشو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3231</th>\n",
       "      <td>79</td>\n",
       "      <td>79 کتابخانه منفرد</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3232 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            pred                      target\n",
       "0                          socal                       socal\n",
       "1                       ویرجینیا        شرکت اوهایو ویرجینیا\n",
       "2                      22 نوامبر                   22 نوامبر\n",
       "3               نورمن ها و نورمن                       نورمن\n",
       "4                           دلتا                  راین قدیمی\n",
       "...                          ...                         ...\n",
       "3227  منطقه ای در جنوب شرق اروپا  منطقه ای در جنوب شرق اروپا\n",
       "3228          باکتریهای بی هوازی          باکتریهای بی هوازی\n",
       "3229                    دهه 1960                    دهه 1960\n",
       "3230                        ورشو                        ورشو\n",
       "3231                          79           79 کتابخانه منفرد\n",
       "\n",
       "[3232 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "dS7rqD_MKcpc"
   },
   "outputs": [],
   "source": [
    "d_test.to_csv('./model/outs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4Uq-hKqzK-DU"
   },
   "outputs": [],
   "source": [
    "d_test = pd.read_csv('./model/outs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "\n",
    "def compute_f1(prediction, answer):\n",
    "    pred_tokens = prediction.split()\n",
    "    answer_tokens = answer.split()\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(answer_tokens) == 0:\n",
    "        return int(pred_tokens == answer_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(answer_tokens)\n",
    "    \n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "    \n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(answer_tokens)\n",
    "    \n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "    \n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(answer_tokens)\n",
    "    \n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "def compute_exact_match(prediction, answer):\n",
    "    return int(prediction == answer)\n",
    "\n",
    "def bleu(prediction, answer) : \n",
    "  reference = [answer.split(' ')]\n",
    "  candidate = pred.split(' ')\n",
    "  BLEU = sentence_bleu(reference, candidate)\n",
    "  BLEU1 = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "  BLEU4 = sentence_bleu(reference, candidate, weights=(0, 0, 0, 1))\n",
    "\n",
    "  return BLEU, BLEU1, BLEU4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EH = 0\n",
    "F1 = 0\n",
    "BLEU = 0\n",
    "BLEU1 = 0\n",
    "BLEU4 = 0\n",
    "for idx, row in d_test.iterrows():\n",
    "    target = row['target']\n",
    "    pred = row['pred']\n",
    "    if target == '':\n",
    "        if pred == 'بدون پاسخ':\n",
    "            EH += 1\n",
    "            F1 += 1\n",
    "            BLEU += 1\n",
    "            BLEU1 += 1\n",
    "            BLEU4 += 1\n",
    "        continue\n",
    "\n",
    "    EH += compute_exact_match(pred, target)\n",
    "    F1 += compute_f1(pred, target)\n",
    "    b, b1, b4 = bleu(pred, target)\n",
    "    BLEU += b\n",
    "    BLEU1 += b1\n",
    "    BLEU4 += b4\n",
    "\n",
    "EH /= len(d_test)\n",
    "F1 /= len(d_test)\n",
    "BLEU /= len(d_test)\n",
    "BLEU1 /= len(d_test)\n",
    "BLEU4 /= len(d_test)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match of ParsT5 on testset : 0.41646039603960394\n",
      "F1 score of ParsT5 on testset : 0.5844615579920778\n",
      "BLEU1 score of ParsT5 on testset : 0.5331487150009248\n"
     ]
    }
   ],
   "source": [
    "print('Exact match of ParsT5 on testset : ' + str(EH))\n",
    "print('F1 score of ParsT5 on testset : ' + str(F1))\n",
    "# print('BLEU score of ParsT5 on testset : ' + str(BLEU))\n",
    "print('BLEU1 score of ParsT5 on testset : ' + str(BLEU1))\n",
    "# print('BLEU4 score of ParsT5 on testset : ' + str(BLEU4))\n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02236697a1c742c39aa921bfde36e8c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b396aabd83374914874d02f0e50acfd9",
      "placeholder": "​",
      "style": "IPY_MODEL_74fbcec5dec04d84b906aa41f7b209b6",
      "value": " 4502/4502 [51:13&lt;00:00,  1.46it/s, loss=0.934, v_num=0]"
     }
    },
    "0d325e1310554dcd9ec1621da7aad860": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_86c7ae40504a4ea484d4559c8eabd6a1",
      "max": 4502,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_daad5d1dbd3a463abc9769a402e78326",
      "value": 4502
     }
    },
    "1eb7e082e87a4391af2f1709b922026e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d835b21a1f8643d4afb32513abe0adb7",
      "placeholder": "​",
      "style": "IPY_MODEL_ea1d3177d5c6489f974569763842a8d0",
      "value": "Epoch 0: 100%"
     }
    },
    "2b8905ab953943ba8bf02099a8ce2382": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "74fbcec5dec04d84b906aa41f7b209b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "86c7ae40504a4ea484d4559c8eabd6a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e62ca451e9146d9a754f43a782fae63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1eb7e082e87a4391af2f1709b922026e",
       "IPY_MODEL_0d325e1310554dcd9ec1621da7aad860",
       "IPY_MODEL_02236697a1c742c39aa921bfde36e8c8"
      ],
      "layout": "IPY_MODEL_2b8905ab953943ba8bf02099a8ce2382"
     }
    },
    "b396aabd83374914874d02f0e50acfd9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d835b21a1f8643d4afb32513abe0adb7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "daad5d1dbd3a463abc9769a402e78326": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ea1d3177d5c6489f974569763842a8d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
